{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; color: Blue; font-family: verdana; font-size: 40px;\">First LLM-based Solution: LangChain & LLaMa</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Pre-requisite (Libraries)](#toc1_)    \n",
    "- [Core Concepts of LangChain:](#toc2_)    \n",
    "  - [Components and Modules](#toc2_1_)    \n",
    "  - [Integration with LLMs](#toc2_2_)    \n",
    "  - [Workflow Management](#toc2_3_)    \n",
    "- [Key LLM-based applications using LangChain](#toc3_)    \n",
    "  - [Chatbots and Virtual Assistants](#toc3_1_)    \n",
    "  - [Retrieval-Augmented Generation (RAG)](#toc3_2_)    \n",
    "  - [Language Translation Tools](#toc3_3_)    \n",
    "  - [Sentiment Analysis Tools](#toc3_4_)    \n",
    "  - [Content Generation](#toc3_5_)    \n",
    "  - [Code Generation and Assistance](#toc3_6_)    \n",
    "  - [Personalized Learning Assistants](#toc3_7_)    \n",
    "  - [Data Analysis and Visualization](#toc3_8_)    \n",
    "- [Building a Chatbot](#toc4_)    \n",
    "  - [Initialize the OpenAI Chat Model](#toc4_1_)    \n",
    "  - [Initialize the LMaMa (Large Language Model Meta AI) Chat Model](#toc4_2_)    \n",
    "  - [Create a Function for Chatting](#toc4_3_)    \n",
    "  - [Run the Chatbot](#toc4_4_)    \n",
    "- [Retrieval-Augmented Generation (RAG)](#toc5_)    \n",
    "  - [Document Store](#toc5_1_)    \n",
    "  - [Embedding Model](#toc5_2_)    \n",
    "  - [Retriever](#toc5_3_)    \n",
    "  - [Language Model (LLM)](#toc5_4_)    \n",
    "  - [RAG Chain](#toc5_5_)    \n",
    "  - [Query Interface](#toc5_6_)    \n",
    "  - [Example Workflow](#toc5_7_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Pre-requisite (Libraries)](#toc0_)\n",
    "\n",
    "1. Setup your virtual environment:\n",
    "\n",
    "`python3 -m venv .venv`\n",
    "\n",
    "2. Activate your virtual environment:\n",
    "\n",
    "`source .venv/bin/activate`\n",
    "\n",
    "3. install LangChain\n",
    "\n",
    "`pip install langchain`\n",
    "\n",
    "4. Install needed Libraries for your LLMs: Integration of LLM models in LangChain. API key should be provided. You can use your OpenAI account to generate a key and pass it as a variable when initiating the OpenAI LLM class. Or you can use Ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using OpenAI, make your API key available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# access variable\n",
    "\n",
    "api_key = os.environ.get(\"my_openai_key\")\n",
    "\n",
    "# print(f'API Key: {api_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Core Concepts of LangChain:](#toc0_)\n",
    "\n",
    "LangChain is built on the concept of components and chains. \n",
    "1. **<u>Components</u>**: are the reusable modules that perform specific tasks such as processing input data, generating text formats, accessing external information or managing workflows.\n",
    "\n",
    "2. **<u>Chains</u>**: are sequences of components that work together to form and achieve your goal such as generating text formats, summarizing a document or providing specific tailored recommendations.\n",
    "\n",
    "\n",
    "## <a id='toc2_1_'></a>[Components and Modules](#toc0_)\n",
    "\n",
    "In LangChain the terms `components` and `modules` are sometimes used interchangeably. But there is an essential difference between both terms.\n",
    "\n",
    "1. **<u>Components</u>** are the core building blocks of LangChain. They represent specific tasks or functionalities. Typically small and specific, they are reused across different applications and/or workflows.\n",
    "\n",
    "2. **<u>Module</u>** combine a set of components to achieve more complex functionalities. LangChain provides standard interfaces for for some of its main modules such as memory and agents modules.\n",
    "\n",
    "Both components and modules are reusable and can be combines together in workflows. This is called a chain where sequences of components or modules are put together to perform a task.\n",
    "\n",
    "## <a id='toc2_2_'></a>[Integration with LLMs](#toc0_)\n",
    "\n",
    "LangChain seamlessly integrates with different LLMs by making available standardized interface. besides the standard connection mechanism to LLMs, LangChain offers multiple features to optimize the use of those models (LLMs) to build language-based applications:\n",
    "\n",
    "1. **<u>Prompt Management:</u>** LangChain allows to craft effective prompts that help the LLMs understand the task and generate useful response(s).\n",
    "\n",
    "2. **<u>Dynamic LLM Selection:</u>** It provides the possibility to select the most suitable model for a given task (given the complexity, the accuracy criteria, computation capability etc..).\n",
    "\n",
    "3. **<u>Memory Management:</u>** LangChain integrates with memory modules allowing LLMs to access and process external information.\n",
    "\n",
    "4. **<u>Agent-based Management:</u>** LangChain enables the orchestration of complex LLM-based workflows that could adapt te condition changes (such as user needs)\n",
    "\n",
    "## <a id='toc2_3_'></a>[Workflow Management](#toc0_)\n",
    "\n",
    "It is the process of orchestrating and controlling the execution of chains and agents to solve specific problem. This deals essentially with managing the data flow, coordinating the execution of components and/or modules, and making sure that the crafted solution is responding effectively to user interactions and changing circumstances.\n",
    "\n",
    "1. **<u>Chain orchestration:</u>** LangChain coordinates the execution of chains to ensure tasks are performed in the correct order and data is correctly passed between components.\n",
    "\n",
    "2. **<u>Agent-based management:</u>** The use of agents is simplified with predefined templates and a user-friendly interface.\n",
    "\n",
    "3. **<u>State management:</u>** LangChain automatically tracks the state of the application, providing developers with a unified interface for accessing and modifying state information.\n",
    "\n",
    "4. **<u>Concurrency management:</u>** LangChain handles the complexities of concurrent execution, enabling developers to focus on the tasks and interactions without worrying about threading or synchronization issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Key LLM-based applications using LangChain](#toc0_)\n",
    "\n",
    "The most popular use cases for LLM-based applications using LangChain, particularly in open-source contexts:\n",
    "\n",
    "## <a id='toc3_1_'></a>[Chatbots and Virtual Assistants](#toc0_)\n",
    "Description: These applications leverage LLMs to engage users in natural language conversations, providing support, answering questions, or assisting with tasks.\n",
    "Example: Customer service bots that can handle inquiries and provide information based on user input.\n",
    "\n",
    "## <a id='toc3_2_'></a>[Retrieval-Augmented Generation (RAG)](#toc0_)\n",
    "Description: This approach combines document retrieval with LLM capabilities, allowing users to “chat” with their documents. It retrieves relevant information from a database or document store and generates responses based on that data.\n",
    "Example: Applications that allow users to ask questions about specific documents, such as PDFs or web pages, and receive contextually relevant answers.\n",
    "\n",
    "## <a id='toc3_3_'></a>[Language Translation Tools](#toc0_)\n",
    "Description: LLMs can be used to translate text between languages, providing real-time translation services.\n",
    "Example: Applications that facilitate communication between speakers of different languages in chat or messaging platforms.\n",
    "\n",
    "## <a id='toc3_4_'></a>[Sentiment Analysis Tools](#toc0_)\n",
    "Description: These applications analyze text to determine the sentiment (positive, negative, neutral) expressed within it, useful for businesses to gauge customer feedback.\n",
    "Example: Tools that analyze social media posts or customer reviews to provide insights into public sentiment about a brand or product.\n",
    "\n",
    "## <a id='toc3_5_'></a>[Content Generation](#toc0_)\n",
    "Description: LLMs can generate various types of content, including articles, marketing copy, and creative writing.\n",
    "Example: Automated blog post generators that create content based on specified topics or keywords.\n",
    "\n",
    "## <a id='toc3_6_'></a>[Code Generation and Assistance](#toc0_)\n",
    "Description: LLMs can assist developers by generating code snippets or providing explanations for programming concepts.\n",
    "Example: Tools that help with debugging or suggest code improvements based on user queries.\n",
    "\n",
    "## <a id='toc3_7_'></a>[Personalized Learning Assistants](#toc0_)\n",
    "Description: These applications provide tailored educational content and support based on individual learning needs and preferences.\n",
    "Example: Tutoring systems that adapt to a student’s progress and provide resources accordingly.\n",
    "\n",
    "## <a id='toc3_8_'></a>[Data Analysis and Visualization](#toc0_)\n",
    "Description: LLMs can help analyze data and generate visualizations based on user queries.\n",
    "Example: Applications that allow users to ask questions about datasets and receive visual representations of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Building a Chatbot](#toc0_)\n",
    "\n",
    "Using LangChain and OpenAI/Llama (Ollama).\n",
    "\n",
    "Two key components from the LangChain are used to facilitate interactions between users and the language model:\n",
    "\n",
    "1. HumanMessage\n",
    "Purpose: This class represents a message sent by a human user in the conversation. It encapsulates the content of the message and any relevant metadata.\n",
    "Usage: When you create an instance of HumanMessage, you typically pass the user’s input (the text they typed) to it. This helps the model understand that this message is coming from the user and allows it to process the input accordingly.\n",
    "2. AIMessage\n",
    "Purpose: This class represents a message generated by the AI (the language model). Similar to HumanMessage, it contains the content of the message along with any associated metadata.\n",
    "Usage: When the model generates a response, you create an instance of AIMessage with the output text. This distinguishes the AI’s responses from the user’s inputs, maintaining clarity in the conversation flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OpenAI if you have credit or use Llama open source model by installing Ollama which is a free and open-source tool designed for running large language models (LLMs) locally on your computer [Ollama](https://ollama.com/download)\n",
    "\n",
    "* Make sure Ollama server is running: `ollama serve`\n",
    "* Pull the model that you will be using: `ollama pull <model-name>`\n",
    "* to kill or remove one of the models: `ollama rm <model-name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Initialize the OpenAI Chat Model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/vkwqcy6s4f3by68x30pqv01m0000gp/T/ipykernel_58399/3277028069.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key =api_key )\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key =api_key )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Initialize the LMaMa (Large Language Model Meta AI) Chat Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/vkwqcy6s4f3by68x30pqv01m0000gp/T/ipykernel_58399/2305760226.py:4: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(openai_api_key=api_key)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "#initialize the OpenAI model\n",
    "llm = OpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_ollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOllama  \u001b[38;5;66;03m# Updated import statement\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, AIMessage\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the LLaMa model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_ollama'"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama  # Updated import statement\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "# Initialize the LLaMa model\n",
    "model = ChatOllama(model=\"llama2\")  # Specify the model you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Create a Function for Chatting](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def chat_with_bot(user_input):\n",
    "    global conversation_history\n",
    "    human_message = HumanMessage(user_input)\n",
    "    # Append the user message to the conversation history\n",
    "    conversation_history.append(human_message)\n",
    "    \n",
    "    # Invoke the model with the conversation history\n",
    "    response = model.invoke(conversation_history)\n",
    "    \n",
    "    # Append the model response to the conversation history\n",
    "    conversation_history.append(AIMessage(content=response.content))\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[Run the Chatbot](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The capital of Tunisia is Tunis.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run the Chatbot\n",
    "import time\n",
    "\n",
    "# Inside your chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    response = chat_with_bot(user_input)\n",
    "    print(f\"Bot: {response}\")\n",
    "    time.sleep(1)  # Wait for 1 second before the next request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot_user(user_input):\n",
    "    global conversation_history\n",
    "    # Append the user's message to the conversation history\n",
    "    conversation_history.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    # Prepare the messages for the model\n",
    "    messages = [msg.content for msg in conversation_history]  # Only get the content\n",
    "    \n",
    "    # Generate a response from the model\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Append the model's response to the conversation history\n",
    "    conversation_history.append(AIMessage(content=response.content))\n",
    "    \n",
    "    # Return formatted output\n",
    "    return f\"You: {user_input}\\nBot: {response.content}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: what's the capital of tunisia?\n",
      "Bot: \n",
      "The answer to your question is \"Tunis\".\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run the Chatbot\n",
    "import time\n",
    "\n",
    "# Inside your chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    response = chat_with_bot_user(user_input)\n",
    "    print(f\"{response}\")\n",
    "    time.sleep(1)  # Wait for 1 second before the next request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Retrieval-Augmented Generation (RAG)](#toc0_)\n",
    "\n",
    "It combines retrieval and generation capabilities to enhance the performance of the language models. The key Building Blocks are:\n",
    "\n",
    "## <a id='toc5_1_'></a>[Document Store](#toc0_)\n",
    "\n",
    "1. Purpose: A repository of documents that the model can retrieve information from.\n",
    "2. Implementation: You can use various document loaders (like TextLoader in Langchain) to load and manage your documents.\n",
    "\n",
    "## <a id='toc5_2_'></a>[Embedding Model](#toc0_)\n",
    "\n",
    "1. Purpose: Converts documents and queries into vector representations, allowing for efficient similarity searches.\n",
    "2. Implementation: Use models like OpenAIEmbeddings to create embeddings for your documents.\n",
    "\n",
    "## <a id='toc5_3_'></a>[Retriever](#toc0_)\n",
    "\n",
    "1. Purpose: Retrieves relevant documents based on the user’s query.\n",
    "2. Implementation: Langchain provides retrievers like SimpleRetriever, which uses the embeddings to find the most relevant documents.\n",
    "\n",
    "## <a id='toc5_4_'></a>[Language Model (LLM)](#toc0_)\n",
    "\n",
    "1. Purpose: Generates responses based on the retrieved documents.\n",
    "2. Implementation: The Llama model (e.g., Llama(model_name='meta-llama/llama-3-7b')) serves as the generative component, taking context from the retrieved documents to produce coherent answers.\n",
    "\n",
    "## <a id='toc5_5_'></a>[RAG Chain](#toc0_)\n",
    "\n",
    "1. Purpose: Integrates the retriever and the language model into a single workflow.\n",
    "2. Implementation: Use LLMChain in Langchain to connect the retriever and the LLM, allowing for seamless querying and response generation.\n",
    "\n",
    "## <a id='toc5_6_'></a>[Query Interface](#toc0_)\n",
    "\n",
    "1. Purpose: Allows users to input their queries and receive responses.\n",
    "2. Implementation: This can be a simple command-line interface or a more complex web interface, depending on your application.\n",
    "\n",
    "## <a id='toc5_7_'></a>[Example Workflow](#toc0_)\n",
    "\n",
    "1. User Input: The user submits a query.\n",
    "2. Retrieval: The retriever fetches relevant documents based on the query.\n",
    "3. Generation: The LLM generates a response using the retrieved documents as context.\n",
    "4. Output: The response is presented to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import Ollama\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma  # Example vector store\n",
    "from langchain.chains import StuffDocumentsChain, RetrievalQAWithSourcesChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "loader = TextLoader('rag_docs.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding model using Ollama\n",
    "embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to install ChromaDB, an open-source vector store\n",
    "`pip install chromadb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store (ChromaDB is an open-source vector store)\n",
    "# do not forget to pull the embedding in ollama using the command `ollama pull mxbai-embed-large` in terminal\n",
    "vector_store = Chroma.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Ollama model for response generation\n",
    "ollama_model = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(input_variables=[\"context\", \"question\"], template=\"Context: {context}\\nQuestion: {question}\\nAnswer:\")\n",
    "\n",
    "# Create an LLMChain with the Ollama model and the prompt template\n",
    "llm_chain = LLMChain(llm=ollama_model, prompt=prompt_template)\n",
    "\n",
    "# Create a chain to combine documents\n",
    "combine_documents_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"context\")\n",
    "\n",
    "# Create the RAG chain using RetrievalQAWithSourcesChain\n",
    "rag_chain = RetrievalQAWithSourcesChain(combine_documents_chain=combine_documents_chain, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Context: RAG models combine retrieval and generation for better responses.\n",
      "Question: What is the significance of RAG models?\n",
      "Answer:\n",
      "answer: The significance of RAG models lies in their ability to combine the strengths of both retrieval systems and generative models, providing more accurate and contextually relevant responses compared to using either approach alone. By retrieving relevant documents from a knowledge base and augmenting them with generative capabilities, RAG models can provide faster and more personalized responses in applications such as chatbots, where understanding user intent and providing precise information is crucial. Additionally, the integration of RAG models into everyday applications will likely enhance user experiences across multiple domains.\n",
      "source: \n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "context = \"RAG models combine retrieval and generation for better responses.\"\n",
    "question = \"What is the significance of RAG models?\"\n",
    "\n",
    "# Generate the prompt\n",
    "formatted_prompt = prompt_template.format(context=context, question=question)\n",
    "response = rag_chain(formatted_prompt)\n",
    "\n",
    "# print prompt\n",
    "print(f\"Prompt:\\n{formatted_prompt}\")\n",
    "\n",
    "# Print the answer\n",
    "print(f\"answer: {response['answer']}\")  # Access the answer directly\n",
    "print(f\"source: {response['sources']}\")  # Optionally print the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: [Document(metadata={'source': 'rag_docs.txt'}, page_content='# The Significance of RAG Models\\n\\nRetrieval-Augmented Generation (RAG) models combine the strengths of retrieval systems and generative models. By retrieving relevant documents from a knowledge base, RAG models can provide more accurate and contextually relevant responses. This approach is particularly useful in applications like chatbots, where understanding user intent and providing precise information is crucial.\\n\\n# Applications of RAG Models\\n\\nRAG models are widely used in various fields, including customer support, education, and content creation. In customer support, they can quickly retrieve information from FAQs and manuals, allowing for faster response times. In education, RAG models can assist students by providing relevant resources and explanations based on their queries.\\n\\n# Future of RAG Technology\\n\\nAs AI technology continues to evolve, RAG models are expected to become more sophisticated. Future advancements may include improved retrieval algorithms and more powerful generative models, leading to even more accurate and helpful AI systems. The integration of RAG models into everyday applications will likely enhance user experiences across multiple domains.'), Document(metadata={'source': 'rag_docs.txt'}, page_content='# The Significance of RAG Models\\n\\nRetrieval-Augmented Generation (RAG) models combine the strengths of retrieval systems and generative models. By retrieving relevant documents from a knowledge base, RAG models can provide more accurate and contextually relevant responses. This approach is particularly useful in applications like chatbots, where understanding user intent and providing precise information is crucial.\\n\\n# Applications of RAG Models\\n\\nRAG models are widely used in various fields, including customer support, education, and content creation. In customer support, they can quickly retrieve information from FAQs and manuals, allowing for faster response times. In education, RAG models can assist students by providing relevant resources and explanations based on their queries.\\n\\n# Future of RAG Technology\\n\\nAs AI technology continues to evolve, RAG models are expected to become more sophisticated. Future advancements may include improved retrieval algorithms and more powerful generative models, leading to even more accurate and helpful AI systems. The integration of RAG models into everyday applications will likely enhance user experiences across multiple domains.')]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve documents manually for debugging\n",
    "retrieved_docs = retriever.get_relevant_documents(formatted_prompt)\n",
    "print(f\"Retrieved Documents: {retrieved_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;  font-family: verdana; font-size: 16px;\">Happy Python Coding & AI Development!</p>\n",
    "\n",
    "<p style=\"text-align: right; color: Blue; font-family: verdana; font-size: 20px;\">© 2024 Drs. Rafik Borji & Egidio Marotta.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
